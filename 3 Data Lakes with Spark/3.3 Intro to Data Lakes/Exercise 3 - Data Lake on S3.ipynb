{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Data Lake on S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### access S3 from Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that your AWS credentials are loaded as env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open('aws/credentials.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create spark session with hadoop-aws package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3a://udacity-dend/pagila/payment/payment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|payment_id;custom...|\n",
      "|16050;269;2;7;1.9...|\n",
      "|16051;269;1;98;0....|\n",
      "|16052;269;2;678;6...|\n",
      "|16053;269;2;703;0...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer schema, fix header and separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3a://udacity-dend/pagila/payment/payment.csv\",sep=\";\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- staff_id: integer (nullable = true)\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_date: string (nullable = true)\n",
      "\n",
      "+----------+-----------+--------+---------+------+-----------------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|payment_date                 |\n",
      "+----------+-----------+--------+---------+------+-----------------------------+\n",
      "|16050     |269        |2       |7        |1.99  |2017-01-24 21:40:19.996577+00|\n",
      "|16051     |269        |1       |98       |0.99  |2017-01-25 15:16:50.996577+00|\n",
      "|16052     |269        |2       |678      |6.99  |2017-01-28 21:44:14.996577+00|\n",
      "|16053     |269        |2       |703      |0.99  |2017-01-29 00:58:02.996577+00|\n",
      "|16054     |269        |1       |750      |4.99  |2017-01-29 08:10:06.996577+00|\n",
      "+----------+-----------+--------+---------+------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the data yourself "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- staff_id: integer (nullable = true)\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_date: timestamp (nullable = true)\n",
      "\n",
      "+----------+-----------+--------+---------+------+--------------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|payment_date              |\n",
      "+----------+-----------+--------+---------+------+--------------------------+\n",
      "|16050     |269        |2       |7        |1.99  |2017-01-24 21:40:19.996577|\n",
      "|16051     |269        |1       |98       |0.99  |2017-01-25 15:16:50.996577|\n",
      "|16052     |269        |2       |678      |6.99  |2017-01-28 21:44:14.996577|\n",
      "|16053     |269        |2       |703      |0.99  |2017-01-29 00:58:02.996577|\n",
      "|16054     |269        |1       |750      |4.99  |2017-01-29 08:10:06.996577|\n",
      "+----------+-----------+--------+---------+------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import  pyspark.sql.functions as F\n",
    "dfPayment = df.withColumn(\"payment_date\", F.to_timestamp(\"payment_date\"))\n",
    "dfPayment.printSchema()\n",
    "dfPayment.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+---------+------+--------------------+-----+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|        payment_date|month|\n",
      "+----------+-----------+--------+---------+------+--------------------+-----+\n",
      "|     16050|        269|       2|        7|  1.99|2017-01-24 21:40:...|    1|\n",
      "|     16051|        269|       1|       98|  0.99|2017-01-25 15:16:...|    1|\n",
      "|     16052|        269|       2|      678|  6.99|2017-01-28 21:44:...|    1|\n",
      "|     16053|        269|       2|      703|  0.99|2017-01-29 00:58:...|    1|\n",
      "|     16054|        269|       1|      750|  4.99|2017-01-29 08:10:...|    1|\n",
      "+----------+-----------+--------+---------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPayment = dfPayment.withColumn(\"month\", F.month(\"payment_date\"))\n",
    "dfPayment.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute aggregate revenue per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|month|           revenue|\n",
      "+-----+------------------+\n",
      "|    4|28559.460000003943|\n",
      "|    3|23886.560000002115|\n",
      "|    2| 9631.879999999608|\n",
      "|    1| 4824.429999999856|\n",
      "|    5|  514.180000000001|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPayment.createOrReplaceTempView(\"payment\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT month, sum(amount) as revenue\n",
    "    FROM payment\n",
    "    GROUP by month\n",
    "    order by revenue desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date\n",
    "paymentSchema = R([\n",
    "    Fld(\"payment_id\",Int()),\n",
    "    Fld(\"customer_id\",Int()),\n",
    "    Fld(\"staff_id\",Int()),\n",
    "    Fld(\"rental_id\",Int()),\n",
    "    Fld(\"amount\",Dbl()),\n",
    "    Fld(\"payment_date\",Date()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give explicit schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPaymentWithSchema = spark.read.csv(\"s3a://udacity-dend/pagila/payment/payment.csv\",sep=\";\", schema=paymentSchema, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- staff_id: integer (nullable = true)\n",
      " |-- rental_id: integer (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- payment_date: date (nullable = true)\n",
      "\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "|payment_id|customer_id|staff_id|rental_id|amount|        payment_date|\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "|     16050|        269|       2|        7|  1.99|2017-01-24 21:40:...|\n",
      "|     16051|        269|       1|       98|  0.99|2017-01-25 15:16:...|\n",
      "|     16052|        269|       2|      678|  6.99|2017-01-28 21:44:...|\n",
      "|     16053|        269|       2|      703|  0.99|2017-01-29 00:58:...|\n",
      "|     16054|        269|       1|      750|  4.99|2017-01-29 08:10:...|\n",
      "+----------+-----------+--------+---------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPaymentWithSchema.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|  m|           revenue|\n",
      "+---+------------------+\n",
      "|  4|28559.460000003943|\n",
      "|  3|23886.560000002115|\n",
      "|  2| 9631.879999999608|\n",
      "|  1| 4824.429999999856|\n",
      "|  5|  514.180000000001|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPaymentWithSchema.createOrReplaceTempView(\"payment\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT month(payment_date) as m, sum(amount) as revenue\n",
    "    FROM payment\n",
    "    GROUP by m\n",
    "    order by revenue desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data = \"s3a://udacity-dend/song_data/A/A/A/*.json\"\n",
    "df_song = spark.read.json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: integer (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "+------------------+---------------+--------------------+----------------+------------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|       artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+--------------------+----------------+------------------+---------+---------+------------------+--------------------+----+\n",
      "|ARTC1LV1187B9A4858|        51.4536|Goldsmith's Colle...|        -0.01802|The Bonzo Dog Band|301.40036|        1|SOAFBCP12A8C13CC7D|King Of Scurf (20...|1972|\n",
      "+------------------+---------------+--------------------+----------------+------------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_song.printSchema()\n",
    "df_song.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df_song = df_song.withColumn(\"num_songs\", expr(\"cast(num_songs as int)\"))\n",
    "df_song = df_song.withColumn(\"year\", expr(\"cast(year as int)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+------------------+----+---------+\n",
      "|           song_id|       title|         artist_id|year| duration|\n",
      "+------------------+------------+------------------+----+---------+\n",
      "|SOHKNRJ12A6701D1F8|Drop of Rain|AR10USD1187B99F3F1|   0|189.57016|\n",
      "+------------------+------------+------------------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_table = df_song.select(\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\").dropDuplicates()\n",
    "songs_table.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_table.write.partitionBy(\"year\", \"artist_id\").parquet(\"songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+---------------+--------+---------+\n",
      "|         artist_id|         name|       location|latitude|longitude|\n",
      "+------------------+-------------+---------------+--------+---------+\n",
      "|ARSVTNL1187B992A91|Jonathan King|London, England|51.50632| -0.12714|\n",
      "+------------------+-------------+---------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_table = df_song.selectExpr(\"artist_id as artist_id\", \"artist_name as name\", \"artist_location as location\", \"artist_latitude as latitude\", \"artist_longitude as longitude\").dropDuplicates()\n",
    "artists_table.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_table.write.partitionBy(\"artist_id\").parquet(\"artists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = 's3a://udacity-dend/log_data/2018/11/2018-11-3*.json'\n",
    "df_log = spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: integer (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: integer (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- new_ts: string (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      "\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+--------------------+-------------------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|                song|status|           ts|           userAgent|userId|              new_ts|         start_time|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+--------------------+-------------------+\n",
      "|A Fine Frenzy|Logged In| Anabelle|     F|            0| Simpson|267.91138| free|Philadelphia-Camd...|   PUT|NextSong|1.541044398796E12|      256|Almost Lover (Alb...|   200|1541377992796|\"Mozilla/5.0 (Mac...|    69|2018-11-05 00:33:...|2018-11-05 00:33:12|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+--------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_log.printSchema()\n",
    "df_log.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df_log.where(df_log.page == \"NextSong\")\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "df_log = df_log.withColumn(\"itemInSession\", expr(\"cast(itemInSession as int)\"))\n",
    "df_log = df_log.withColumn(\"sessionId\", expr(\"cast(sessionId as int)\"))\n",
    "df_log = df_log.withColumn(\"status\", expr(\"cast(status as int)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "+-------+----------+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_table = df_log.selectExpr(\"userId as user_id\", \"firstName as first_name\", \"lastName as last_name\", \"gender as gender\", \"level as level\").dropDuplicates()\n",
    "users_table.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table.write.partitionBy(\"user_id\").parquet(\"users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_timestamp = udf(lambda x: datetime.fromtimestamp(x / 1000.0).strftime('%Y-%m-%d %H:%M:%S.%f')) \n",
    "df_log = df_log.withColumn(\"new_ts\", get_timestamp(df_log.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x / 1000.0).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "df_log = df_log.withColumn(\"start_time\", get_datetime(df_log.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|         start_time|hour|day|week|month|year|weekday|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|2018-11-05 00:33:12|   0|  5|  45|   11|2018|      2|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table = df_log.selectExpr(\"start_time as start_time\", \"hour(start_time) as hour\", \"dayofmonth(start_time) as day\", \"weekofyear(start_time) as week\", \"month(start_time) as month\", \"year(start_time) as year\", \"dayofweek(start_time) as weekday\")\n",
    "time_table.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table.write.partitionBy(\"year\", \"month\").parquet(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-----+-------+---------+---------+--------------------+--------------------+-----------+\n",
      "|         start_time|userId|level|song_id|artist_id|sessionId|            location|           userAgent|songplay_id|\n",
      "+-------------------+------+-----+-------+---------+---------+--------------------+--------------------+-----------+\n",
      "|2018-11-30 00:22:07|    91| free|   null|     null|      829|Dallas-Fort Worth...|Mozilla/5.0 (comp...|          0|\n",
      "+-------------------+------+-----+-------+---------+---------+--------------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays_table = df_log \\\n",
    "                .join(df_song, [df_log.artist == df_song.artist_name, df_log.song == df_song.title], 'left') \\\n",
    "                .select(df_log.start_time, \\\n",
    "                        df_log.userId, \\\n",
    "                        df_log.level, \\\n",
    "                        df_song.song_id, \\\n",
    "                        df_song.artist_id, \\\n",
    "                        df_log.sessionId, \\\n",
    "                        df_log.location, \\\n",
    "                        df_log.userAgent)\n",
    "songplays_table = songplays_table.withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "songplays_table.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table = songplays_table.selectExpr(\"songplay_id as songplay_id\", \\\n",
    "                                             \"start_time as start_time\", \\\n",
    "                                             \"userId as user_id\", \\\n",
    "                                             \"level as level\", \\\n",
    "                                             \"song_id as song_id\", \\\n",
    "                                             \"artist_id as artist_id\", \\\n",
    "                                             \"sessionId as session_id\", \\\n",
    "                                             \"location as location\", \\\n",
    "                                             \"userAgent as user_agent\", \\\n",
    "                                             \"year(start_time) as year\", \\\n",
    "                                             \"month(start_time) as month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table.write.partitionBy(\"year\", \"month\").parquet(\"songplays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
